{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy for Rule Updating\n",
    "---\n",
    "#### Concept before Implementation:\n",
    "- When our apriori rules have a high cross entropy, this indicates that during this time period, this rule provides less confidence. \n",
    "- This observation can lead to either updating the rule or telling users that they should wait until there is less noise between these two assets to go ahead and buy, sell, or stay in the current position. \n",
    "- POTENIAL ISSUE: Sliding windows for this will be hard to deal with. How do we know what period we should measure cross-entropy in? \n",
    "- POTENTIAL SOLUTION: Try cross entropy with multiple fixed sized windows. After this, use an attention model such as LTSM to mathematically compute the time window, and finally compare the results of these findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.typing as npt\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def cross_entropy_histogram(\n",
    "    returns_A: npt.NDArray[np.float64],\n",
    "    returns_B: npt.NDArray[np.float64],\n",
    "    epsilon: float = 1e-11\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute cross entropy H(p, q) where p is the distribution of returns_A\n",
    "    and q is the distribution of returns_B, approximated by normalized histograms.\n",
    "    \n",
    "    returns_A, returns_B: 1D arrays of returns in the given time window.\n",
    "    epsilon: small value to avoid log(0).\n",
    "    \"\"\"\n",
    "    n_rows_a = np.shape(returns_A)\n",
    "\n",
    "    bins = int(np.ceil(np.log2(n_rows_a + 1)))\n",
    "    hist_A, bin_edges = np.histogram(returns_A, bins=bins, density=True)\n",
    "    hist_B, _ = np.histogram(returns_B, bins=bin_edges, density=True)\n",
    "\n",
    "    p = hist_A\n",
    "    q = hist_B\n",
    "\n",
    "    ce = -np.sum(p * np.log(q + epsilon))\n",
    "    return ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes with this method: \n",
    "- The return streams, specifically A because it determines bucket size, we need to find a new method to determine window size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ce_matrix(\n",
    "    a: npt.NDArray[np.float64],\n",
    "    b: npt.NDArray[np.float64],\n",
    "    c: npt.NDArray[np.float64],\n",
    "    epsilon: float\n",
    ") -> npt.NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Helper – given three 1‑D windows of equal length, return the 3×3\n",
    "    directed cross‑entropy matrix exactly like your original routine.\n",
    "    \"\"\"\n",
    "    # common bin edges (Sturges on the window size)\n",
    "    n_bins = int(np.ceil(np.log2(len(a) + 1)))\n",
    "    all_data = np.concatenate((a, b, c))\n",
    "    _, edges = np.histogram(all_data, bins=n_bins, density=False)\n",
    "\n",
    "    pA, _ = np.histogram(a, bins=edges, density=True)\n",
    "    pB, _ = np.histogram(b, bins=edges, density=True)\n",
    "    pC, _ = np.histogram(c, bins=edges, density=True)\n",
    "\n",
    "    M = np.empty((3, 3), dtype=np.float64)\n",
    "    M[0, 0] = -np.sum(pA * np.log(pA + epsilon))\n",
    "    M[0, 1] = -np.sum(pA * np.log(pB + epsilon))\n",
    "    M[0, 2] = -np.sum(pA * np.log(pC + epsilon))\n",
    "\n",
    "    M[1, 0] = -np.sum(pB * np.log(pA + epsilon))\n",
    "    M[1, 1] = -np.sum(pB * np.log(pB + epsilon))\n",
    "    M[1, 2] = -np.sum(pB * np.log(pC + epsilon))\n",
    "\n",
    "    M[2, 0] = -np.sum(pC * np.log(pA + epsilon))\n",
    "    M[2, 1] = -np.sum(pC * np.log(pB + epsilon))\n",
    "    M[2, 2] = -np.sum(pC * np.log(pC + epsilon))\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More than 2 assets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @numba.njit\n",
    "def rolling_cross_entropy_histogram_3(\n",
    "    returns_A: npt.NDArray[np.float64],\n",
    "    returns_B: npt.NDArray[np.float64],\n",
    "    returns_C: npt.NDArray[np.float64],\n",
    "    window: int = 7,\n",
    "    epsilon: float = 1e-11\n",
    ") -> npt.NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Rolling cross‑entropy over time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns_A, returns_B, returns_C : 1‑D arrays of equal length T\n",
    "        Daily return series for the three assets.\n",
    "    window : int, default 7\n",
    "        Number of days used to build the empirical distributions.\n",
    "    epsilon : float\n",
    "        Small constant to avoid log(0).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : ndarray, shape (T‑window+1, 3, 3)\n",
    "        result[t] is the 3×3 matrix H_t computed from\n",
    "        days  t‑window+1 … t  (inclusive).\n",
    "    \"\"\"\n",
    "    if not (len(returns_A) == len(returns_B) == len(returns_C)):\n",
    "        raise ValueError(\"All three return arrays must have the same length.\")\n",
    "    if window < 2:\n",
    "        raise ValueError(\"window must be at least 2.\")\n",
    "\n",
    "    T = len(returns_A)\n",
    "    if window > T:\n",
    "        raise ValueError(\"window larger than length of series.\")\n",
    "\n",
    "    out = np.empty((T - window + 1, 3, 3), dtype=np.float64)\n",
    "\n",
    "    for t in range(window - 1, T):\n",
    "        sl = slice(t - window + 1, t + 1)      # rolling window\n",
    "        out[t - window + 1] = _ce_matrix(\n",
    "            returns_A[sl], returns_B[sl], returns_C[sl], epsilon\n",
    "        )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.read_excel(\"Cross_Entropy_Updating.xlsx\", sheet_name= \"analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = data_pd.iloc[:, 1:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rolling_cross_entropy_histogram_3(data_np[:, 0], data_np[:, 1], data_np[:, 2], 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ce_pd = pd.read_excel(\"ce_review_1.xlsx\", sheet_name= \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ce_np = data_ce_pd.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_np(\n",
    "    series: np.ndarray,          # 1‑D array of length T\n",
    "    lookback: int,               # L\n",
    "    horizon: int,                # k\n",
    "    alpha: float                 # quantile for divergence threshold\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    series   : shape (T,)\n",
    "    lookback : number of past days fed to the network\n",
    "    horizon  : number of days ahead used to define the label\n",
    "    alpha    : quantile for the divergence threshold (e.g. 0.80)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray, shape (samples, lookback, 1)\n",
    "    y : ndarray, shape (samples,)  dtype int {0,1}\n",
    "    \"\"\"\n",
    "    series = np.asarray(series, dtype=np.float64)\n",
    "    T = series.size\n",
    "    if T <= lookback + horizon:\n",
    "        raise ValueError(\"series too short for given lookback & horizon\")\n",
    "\n",
    "    # 1) ΔH over the horizon  (length = T‑horizon)\n",
    "    delta = series[horizon:] - series[:-horizon]\n",
    "\n",
    "    # 2) binary label based on α‑quantile\n",
    "    thresh = np.quantile(delta, alpha)\n",
    "    y_full = (delta > thresh).astype(np.int8)        # length T‑horizon\n",
    "\n",
    "    # 3) build rolling windows\n",
    "    X_list, y_list = [], []\n",
    "    for i in range(lookback, T - horizon):           # i is the *current* day\n",
    "        X_list.append(series[i - lookback:i])        # past lookback values\n",
    "        y_list.append(y_full[i])                     # diverge in next k days?\n",
    "\n",
    "    X = np.asarray(X_list, dtype=np.float64)[..., None]  # add channel dim\n",
    "    y = np.asarray(y_list, dtype=np.int8)\n",
    "\n",
    "    return (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOWS = [7, 14, 28, 60]           # same order as the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOKBACK = 30\n",
    "HORIZON  = 5\n",
    "ALPHA    = 0.80\n",
    "\n",
    "datasets = {}\n",
    "for j, w in enumerate(WINDOWS):          # j = column index\n",
    "    X, y = make_dataset_np(\n",
    "        data_ce_np[:, j],                # 1‑D slice for that window length\n",
    "        LOOKBACK,\n",
    "        HORIZON,\n",
    "        ALPHA\n",
    "    )\n",
    "    datasets[w] = (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    val_frac: float = 0.20\n",
    ") -> tuple[tuple[np.ndarray, np.ndarray], tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Deterministic split that keeps the **earlier** 1‑val_frac fraction for\n",
    "    training and the **latest** val_frac fraction for validation.\n",
    "    \"\"\"\n",
    "    split = int(len(X) * (1.0 - val_frac))\n",
    "    return (X[:split], y[:split]), (X[split:], y[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits: dict[int, tuple[tuple[np.ndarray, np.ndarray], tuple[np.ndarray, np.ndarray]]] = {\n",
    "    w: ts_split(*datasets[w]) for w in WINDOWS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(input_shape: tuple) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    input_shape = (LOOKBACK, n_channels)\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.LSTM(32, return_sequences=False),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.AUC(name=\"auc\")]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7-day window →  AUC=0.759   F1=0.104\n",
      "14-day window →  AUC=0.591   F1=0.000\n",
      "28-day window →  AUC=0.745   F1=0.000\n",
      "60-day window →  AUC=0.597   F1=0.000\n"
     ]
    }
   ],
   "source": [
    "LOOKBACK = datasets[WINDOWS[0]][0].shape[1]   # same for all\n",
    "\n",
    "results = {}\n",
    "\n",
    "for w, ((X_tr, y_tr), (X_val, y_val)) in splits.items():\n",
    "    model = build_lstm((LOOKBACK, 1))\n",
    "    es = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "\n",
    "    p_val = model.predict(X_val, verbose=0).ravel()\n",
    "    auc   = roc_auc_score(y_val, p_val)\n",
    "    f1    = f1_score(y_val, (p_val > 0.5).astype(int))\n",
    "\n",
    "    results[w] = {\"AUC\": auc, \"F1\": f1}\n",
    "    print(f\"{w:>2}-day window →  AUC={auc:.3f}   F1={f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_attn(input_shape: tuple, lstm_units: int = 32) -> tf.keras.Model:\n",
    "    n_channels = input_shape[-1]\n",
    "    \n",
    "    seq_in = layers.Input(shape=input_shape)    # shape: (LOOKBACK, n_channels)\n",
    "    \n",
    "    # 1) LSTM encoder on the time dimension\n",
    "    lstm_out = layers.LSTM(lstm_units, return_sequences=False)(seq_in)  # (batch, lstm_units)\n",
    "    \n",
    "    # 2) Channel-wise attention:\n",
    "    # (a) Get the last time step values per channel\n",
    "    last_step = layers.Lambda(lambda t: t[:, -1, :])(seq_in)  # (batch, n_channels)\n",
    "    \n",
    "    # (b) Compute raw attention scores with a Dense layer (name it \"att_dense\")\n",
    "    att_dense = layers.Dense(n_channels, name=\"att_dense\")(last_step)  # (batch, n_channels)\n",
    "    \n",
    "    # (c) Apply softmax to get attention weights (Activation layer)\n",
    "    att_soft = layers.Activation(\"softmax\", name=\"attn\")(att_dense)  # (batch, n_channels)\n",
    "    \n",
    "    # (d) Compute the weighted sum over the input channels\n",
    "    context = layers.Lambda(\n",
    "        lambda x: tf.reduce_sum(x[0] * x[1], axis=1, keepdims=True)\n",
    "    )([last_step, att_soft])  # shape: (batch, 1)\n",
    "    \n",
    "    # 3) Concatenate the LSTM output and the context scalar.\n",
    "    merged = layers.Concatenate()([lstm_out, context])  # (batch, lstm_units + 1)\n",
    "    \n",
    "    # 4) Final classification layer.\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(merged)\n",
    "    \n",
    "    model = models.Model(seq_in, out)\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[tf.keras.metrics.AUC(name=\"auc\")])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.stack([datasets[w][0][..., 0] for w in WINDOWS], axis=-1)\n",
    "y_all = datasets[WINDOWS[0]][1]  # Using the labels from one window (assumed to be identical across windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_split(X, y, val_frac=0.2):\n",
    "    split = int(len(X) * (1 - val_frac))\n",
    "    return (X[:split], y[:split]), (X[split:], y[split:])\n",
    "\n",
    "(X_tr, y_tr), (X_val, y_val) = ts_split(X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention dense weights: [[-0.47787336 -0.7813181   0.40576404  0.27786258]\n",
      " [ 0.50365853 -0.40947962 -0.66947925  0.4415727 ]\n",
      " [ 0.5782949  -0.8263428  -0.6641926  -0.655264  ]\n",
      " [ 0.72733355 -0.6088978  -0.1244367  -0.61146486]]\n"
     ]
    }
   ],
   "source": [
    "# Assuming LOOKBACK and WINDOWS are defined:\n",
    "model = build_lstm_attn((LOOKBACK, len(WINDOWS)))\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# X_all should have shape (samples, LOOKBACK, len(WINDOWS))\n",
    "model.fit(\n",
    "    X_all, y_all,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Inspect the attention weights:\n",
    "# After training the model:\n",
    "att_dense_weights = model.get_layer(\"att_dense\").get_weights()[0]\n",
    "print(\"Attention dense weights:\", att_dense_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Attention distributions for the first 5 samples:\n",
      "[[0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.1773922e-38]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0352684e-37]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1190406e-32]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2712054e-34]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.6068732e-36]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3140769e-26]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 5.5701137e-14 0.0000000e+00 1.0000000e+00]\n",
      " [0.0000000e+00 6.0750152e-14 0.0000000e+00 1.0000000e+00]\n",
      " [9.3916673e-37 0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
      " [2.2225791e-37 7.1192921e-12 0.0000000e+00 1.0000000e+00]\n",
      " [8.3341433e-07 1.6233813e-16 0.0000000e+00 9.9999917e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Build a sub-model to extract attention probabilities from the \"attn\" layer.\n",
    "att_model = tf.keras.Model(inputs=model.input,\n",
    "                           outputs=model.get_layer(\"attn\").output)\n",
    "\n",
    "# Use the sub-model to predict the attention probabilities for your validation set.\n",
    "att_values = att_model.predict(X_val)\n",
    "\n",
    "# For example, print the attention distribution for the first 5 validation samples.\n",
    "print(\"Attention distributions for the first 5 samples:\")\n",
    "print(att_values[:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_df = pd.DataFrame(att_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_df.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ts_split(X: np.ndarray, y: np.ndarray, val_frac: float = 0.20):\n",
    "    split = int(len(X) * (1.0 - val_frac))\n",
    "    return (X[:split], y[:split]), (X[split:], y[split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# Parameters for retraining loop\n",
    "# ------------------------------------------\n",
    "STEP_DAYS  = 20          # retrain every 20 samples (adjust as needed)\n",
    "EXPAND_WIN = True        # if True, use all data up to the retraining point\n",
    "MIN_TRAIN  = 250         # minimal training size (e.g. about one year of data)\n",
    "\n",
    "# Here we assume X_all.shape[0] == number of samples (time steps)\n",
    "# We'll simulate a date index as just the sample indices.\n",
    "dates = np.arange(X_all.shape[0])\n",
    "\n",
    "# Choose cut points starting after MIN_TRAIN samples, stepping every STEP_DAYS.\n",
    "cut_points = dates[MIN_TRAIN::STEP_DAYS]\n",
    "\n",
    "# Dictionary to hold the retrained models (keyed by cut point)\n",
    "models = {}\n",
    "\n",
    "for cut in cut_points:\n",
    "    # ------------------------------------------\n",
    "    # 1) Slice the data up to (but not including) the cut point\n",
    "    # ------------------------------------------\n",
    "    if EXPAND_WIN:\n",
    "        X_cut = X_all[:cut]\n",
    "        y_cut = y_all[:cut]\n",
    "    else:\n",
    "        # For a fixed-length sliding window, adjust the window length (e.g., 500 samples)\n",
    "        window_size = 500\n",
    "        if cut < window_size:\n",
    "            continue\n",
    "        X_cut = X_all[cut - window_size:cut]\n",
    "        y_cut = y_all[cut - window_size:cut]\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 2) Split the data into training and validation sets\n",
    "    # ------------------------------------------\n",
    "    (X_tr, y_tr), (X_val, y_val) = ts_split(X_cut, y_cut, val_frac=0.20)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 3) Build and train the model (using the updated build_lstm_attn)\n",
    "    # ------------------------------------------\n",
    "    model = build_lstm_attn((LOOKBACK, len(WINDOWS)))  # updated attention model\n",
    "    es = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        verbose=0,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 4) Inspect the attention weights via the Dense layer\n",
    "    # ------------------------------------------\n",
    "    # Retrieve weights of the Dense layer named \"att_dense\".\n",
    "    att_dense_weights = model.get_layer(\"att_dense\").get_weights()[0]  # shape: (input_dim, n_channels)\n",
    "    # Depending on the Dense implementation, the weight matrix has shape (input_dim, n_channels).\n",
    "    # We can average the weights over the input dimension (here, input_dim corresponds to the last-step features)\n",
    "    mean_att = att_dense_weights.mean(axis=0)\n",
    "    \n",
    "    print(f\"Cut index {cut:4d} → Mean attention weights per channel: {dict(zip(WINDOWS, np.round(mean_att, 3)))}\")\n",
    "    \n",
    "    # Optionally, you can also create a sub-model to get the actual softmax outputs:\n",
    "    # att_model = tf.keras.Model(model.input, model.get_layer(\"attn\").output)\n",
    "    # att_values = att_model.predict(X_val)\n",
    "    # print(\"Sample attention outputs (first validation sample):\", att_values[0])\n",
    "    \n",
    "    models[cut] = model\n",
    "\n",
    "print(\"Retraining complete. Models are stored in the `models` dictionary keyed by retraining index.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
